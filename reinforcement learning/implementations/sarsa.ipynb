{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gym\nimport collections\nimport numpy as np\ndef discretize(observation, bins):\n    discrete_obs = [np.digitize(obs, bins[i]) for i, obs in enumerate(observation)]\n    return tuple(discrete_obs)\n\ndef choose_action(Q, state, epsilon, env):\n    if np.random.rand() < epsilon:\n        return env.action_space.sample()\n    else:\n        return np.argmax(Q[state])\n\n\ndef update_Q_SARSA(Q, alpha, state, action, reward, gamma, next_state, next_action):\n    state_q = Q[state+(action,)]\n    next_state_q = Q[next_state+(next_action,)]\n    Q[state+(action,)] = state_q + alpha * (reward + gamma * next_state_q-state_q)\n\n\ndef train(env, episodes=50000, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.05, alpha=0.1):\n    num_bins = [6, 12, 12, 24]  # Define the number of bins for each observation\n    bins = [\n        np.linspace(-2.4, 2.4, num_bins[0]),\n        np.linspace(-5, 5, num_bins[1]),\n        np.linspace(-0.418, 0.418, num_bins[2]),\n        np.linspace(-5, 5, num_bins[3])\n    ]\n\n    Q = np.random.uniform(low=0, high=1, size=(num_bins + [env.action_space.n]))\n    rewards = []\n\n    for episode in range(episodes):\n        observation = env.reset()[0]\n        state = discretize(tuple(observation), bins)\n        total_reward = 0\n        done = False\n        action = choose_action(Q, state, epsilon, env)\n        while not done:\n            obs, reward, done, i, j = env.step(action)\n            next_state = discretize(obs, bins)\n            next_action = choose_action(Q, next_state, epsilon,env)\n            update_Q_SARSA(Q, alpha, state, action, reward, gamma, next_state,next_action)\n            state = next_state\n            action = next_action\n            total_reward += reward\n\n        rewards.append(total_reward)\n\n        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n\n\n        if episode >= 100 and np.mean(rewards[-100:]) >= 195.0:\n            print(f\"Solved in {episode + 1} episodes!\")\n            break\n\n        if (episode + 1) % 100 == 0:\n            print(f\"Episode {episode + 1}, Average Reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n\nenv = gym.make('CartPole-v1')\n\ntrain(env)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-13T22:44:38.175613Z","iopub.execute_input":"2024-05-13T22:44:38.176043Z","iopub.status.idle":"2024-05-13T22:44:43.773571Z","shell.execute_reply.started":"2024-05-13T22:44:38.176013Z","shell.execute_reply":"2024-05-13T22:44:43.772278Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Episode 100, Average Reward (last 100 episodes): 27.47\nEpisode 200, Average Reward (last 100 episodes): 33.43\nEpisode 300, Average Reward (last 100 episodes): 35.00\nEpisode 400, Average Reward (last 100 episodes): 53.45\nEpisode 500, Average Reward (last 100 episodes): 91.76\nEpisode 600, Average Reward (last 100 episodes): 124.24\nEpisode 700, Average Reward (last 100 episodes): 115.31\nEpisode 800, Average Reward (last 100 episodes): 167.51\nSolved in 828 episodes!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}